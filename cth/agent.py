"""CTH Agent Module

Implements LLM-driven intelligent diagnosis and remediation based on CTH analysis.
Integrates with the existing KubeAgent framework.
"""

from typing import Dict, List, Any, Optional, Generator
from datetime import datetime
import json

from langchain_core.prompts import PromptTemplate
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_openai import ChatOpenAI

from .graph import CTHGraph, Hyperedge
from .builder import CTHBuilder
from .analyzer import PropagationAnalyzer, PropagationPath


class CTHAgent:
    """LLM-powered agent for CTH-based fault diagnosis and remediation.
    
    Implements the ReAct/RAG framework where CTH serves as the external knowledge base
    that the LLM agent can query and reason about.
    """
    
    def __init__(self, llm: Optional[BaseChatModel] = None):
        """
        Initialize CTH Agent.
        
        Args:
            llm: Language model for reasoning. If None, uses DeepSeek by default.
        """
        if llm is None:
            import os
            llm = ChatOpenAI(
                model="deepseek-chat",
                temperature=0.3,  # Lower temperature for more focused analysis
                base_url="https://api.deepseek.com",
                api_key=os.getenv("DEEPSEEK_API_KEY"),
            )
        
        self.llm = llm
        self.cth_builder = CTHBuilder()
        self.propagation_analyzer = PropagationAnalyzer()
        self.current_cth_graph: Optional[CTHGraph] = None
        
        # Initialize prompts for different analysis tasks
        self._init_prompts()
    
    def _init_prompts(self):
        """Initialize prompt templates for different analysis tasks."""
        
        # Resilience pattern identification prompt
        self.pattern_analysis_prompt = PromptTemplate.from_template(
            """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç«™ç‚¹å¯é æ€§å·¥ç¨‹å¸ˆ(SRE)ã€‚è¯·åˆ†æžä»¥ä¸‹æ•…éšœä¼ æ’­è·¯å¾„æ•°æ®ï¼Œè¯†åˆ«å…¶ä¸­å¯èƒ½å­˜åœ¨çš„éŸ§æ€§ç­–ç•¥æ¨¡å¼åŠå…¶æ½œåœ¨å†²çªã€‚

æ•…éšœä¼ æ’­è·¯å¾„ä¿¡æ¯:
{propagation_paths}

ç›¸å…³çš„å¯è§‚æµ‹æ€§æ•°æ®:
{observability_data}

è¯·é‡ç‚¹åˆ†æžä»¥ä¸‹æ–¹é¢:
1. è¯†åˆ«é‡è¯•(Retry)æ¨¡å¼çš„è¯æ®
2. è¯†åˆ«è¶…æ—¶(Timeout)æœºåˆ¶çš„è§¦å‘
3. è¯†åˆ«ç†”æ–­(Circuit Breaker)æ¨¡å¼
4. è¯†åˆ«èˆ±å£(Bulkhead)éš”ç¦»æ¨¡å¼
5. åˆ†æžè¿™äº›éŸ§æ€§ç­–ç•¥ä¹‹é—´æ˜¯å¦å­˜åœ¨å†²çª

è¯·æä¾›è¯¦ç»†çš„åˆ†æžè¿‡ç¨‹å’Œç»“è®ºï¼ŒåŒ…æ‹¬:
- å‘çŽ°çš„éŸ§æ€§æ¨¡å¼
- æ¨¡å¼å†²çªåˆ†æž
- æ”¹è¿›å»ºè®®
"""
        )
        
        # Interactive diagnosis prompt
        self.diagnosis_prompt = PromptTemplate.from_template(
            """ä½ æ˜¯ä¸€ä¸ªåŸºäºŽCTH(å› æžœ-æ—¶åºè¶…å›¾)çš„æ™ºèƒ½è¯Šæ–­åŠ©æ‰‹ã€‚ä½ å¯ä»¥é€šè¿‡åˆ†æžCTHå›¾æ¥è¯Šæ–­äº‘åŽŸç”Ÿç³»ç»Ÿæ•…éšœã€‚

å½“å‰å‘Šè­¦ä¿¡æ¯:
{alert_info}

CTHå›¾ç»Ÿè®¡ä¿¡æ¯:
{cth_statistics}

ä½ çš„ä»»åŠ¡æ˜¯:
1. åˆ†æžå‘Šè­¦çš„æ ¹æœ¬åŽŸå› 
2. è¿½è¸ªæ•…éšœä¼ æ’­è·¯å¾„
3. è¯„ä¼°å½±å“èŒƒå›´
4. æä¾›ä¿®å¤å»ºè®®

è¯·å¼€å§‹ä½ çš„åˆ†æžã€‚å¦‚æžœéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œè¯·è¯´æ˜Žä½ éœ€è¦ä»€ä¹ˆæ•°æ®ã€‚
"""
        )
        
        # Remediation generation prompt
        self.remediation_prompt = PromptTemplate.from_template(
            """åŸºäºŽä»¥ä¸‹CTHåˆ†æžç»“æžœï¼Œè¯·ç”Ÿæˆå…·ä½“çš„ä¿®å¤æ–¹æ¡ˆå’ŒæŠ¥å‘Šã€‚

æ ¹æœ¬åŽŸå› åˆ†æž:
{root_cause_analysis}

æ•…éšœä¼ æ’­è·¯å¾„:
{propagation_paths}

å½±å“èŒƒå›´åˆ†æž:
{scope_analysis}

è¯·ç”Ÿæˆ:
1. å¯æ‰§è¡Œçš„kubectl/helmå‘½ä»¤æ¥ä¿®å¤é—®é¢˜
2. è¯¦ç»†çš„æ•…éšœæŠ¥å‘Š(é€‚åˆJIRAå·¥å•)
3. é¢„é˜²æŽªæ–½å»ºè®®
4. ç›‘æŽ§æ”¹è¿›å»ºè®®

ç¡®ä¿æ‰€æœ‰å»ºè®®éƒ½æ˜¯å®‰å…¨çš„ã€å¯æ‰§è¡Œçš„ï¼Œå¹¶åŒ…å«å¿…è¦çš„éªŒè¯æ­¥éª¤ã€‚
"""
        )
    
    def build_cth_from_data(self, observability_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Build CTH graph from observability data and return construction results.
        
        Args:
            observability_data: JSON data containing traces, metrics, and logs
        
        Returns:
            Dictionary containing construction results and any validation errors
        """
        # Validate input data
        validation_errors = self.cth_builder.validate_input_data(observability_data)
        if validation_errors:
            return {
                'success': False,
                'errors': validation_errors,
                'cth_graph': None
            }
        
        try:
            # Build CTH graph
            self.current_cth_graph = self.cth_builder.build_cth_from_json(observability_data)
            
            return {
                'success': True,
                'errors': [],
                'cth_graph': self.current_cth_graph.to_dict(),
                'statistics': self.current_cth_graph.get_statistics()
            }
        
        except Exception as e:
            return {
                'success': False,
                'errors': [f"CTH construction failed: {str(e)}"],
                'cth_graph': None
            }
    
    def analyze_resilience_patterns(self, propagation_paths: List[PropagationPath], 
                                  observability_data: Dict[str, Any]) -> str:
        """
        Analyze resilience patterns and conflicts in propagation paths.
        
        Args:
            propagation_paths: List of fault propagation paths
            observability_data: Original observability data for context
        
        Returns:
            LLM analysis of resilience patterns and conflicts
        """
        # Prepare data for LLM analysis
        paths_summary = []
        for i, path in enumerate(propagation_paths):
            paths_summary.append({
                'path_id': i + 1,
                'summary': path.get_path_summary(),
                'hyperedges': [{
                    'nodes': list(edge.nodes),
                    'timestamp': edge.timestamp.isoformat(),
                    'severity': edge.severity,
                    'trace_id': edge.trace_id,
                    'metrics': list(edge.metrics),
                    'logs': list(edge.logs)
                } for edge in path.hyperedges]
            })
        
        # Format observability data for context
        obs_summary = {
            'total_traces': len(observability_data.get('traces', [])),
            'total_metrics': len(observability_data.get('metrics', [])),
            'total_logs': len(observability_data.get('logs', [])),
            'sample_traces': observability_data.get('traces', [])[:3],  # First 3 traces
            'anomalous_metrics': [m for m in observability_data.get('metrics', []) if m.get('is_anomalous')],
            'error_logs': [l for l in observability_data.get('logs', []) if l.get('level', '').lower() in ['error', 'critical']]
        }
        
        # Generate LLM analysis
        prompt = self.pattern_analysis_prompt.format(
            propagation_paths=json.dumps(paths_summary, indent=2, ensure_ascii=False),
            observability_data=json.dumps(obs_summary, indent=2, ensure_ascii=False)
        )
        
        response = self.llm.invoke(prompt)
        return response.content
    
    def interactive_diagnosis(self, alert_info: str) -> Generator[str, None, str]:
        """
        Perform interactive diagnosis using ReAct framework.
        
        Args:
            alert_info: Initial alert information
        
        Yields:
            Intermediate reasoning steps
        
        Returns:
            Final diagnosis result
        """
        if not self.current_cth_graph:
            yield "âŒ é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„CTHå›¾æ•°æ®ã€‚è¯·å…ˆæž„å»ºCTHå›¾ã€‚"
            return "è¯Šæ–­å¤±è´¥ï¼šç¼ºå°‘CTHå›¾æ•°æ®"
        
        # Initial reasoning
        cth_stats = self.current_cth_graph.get_statistics()
        
        initial_prompt = self.diagnosis_prompt.format(
            alert_info=alert_info,
            cth_statistics=json.dumps(cth_stats, indent=2, ensure_ascii=False)
        )
        
        yield "ðŸ¤” å¼€å§‹åˆ†æžå‘Šè­¦ä¿¡æ¯å’ŒCTHå›¾æ•°æ®..."
        
        # Step 1: Initial analysis
        response = self.llm.invoke(initial_prompt)
        initial_analysis = response.content
        yield f"ðŸ“Š åˆæ­¥åˆ†æž:\n{initial_analysis}"
        
        # Step 2: Extract potential anomaly nodes from alert
        anomaly_nodes = self._extract_anomaly_nodes_from_alert(alert_info)
        yield f"ðŸŽ¯ è¯†åˆ«åˆ°æ½œåœ¨å¼‚å¸¸èŠ‚ç‚¹: {anomaly_nodes}"
        
        # Step 3: Find propagation paths for each anomaly node
        all_paths = []
        for node in anomaly_nodes:
            paths = self.propagation_analyzer.find_propagation_paths(self.current_cth_graph, node)
            all_paths.extend(paths)
            if paths:
                yield f"ðŸ” ä»ŽèŠ‚ç‚¹ {node} å‘çŽ° {len(paths)} æ¡ä¼ æ’­è·¯å¾„"
        
        if not all_paths:
            yield "âš ï¸ æœªå‘çŽ°æ˜Žæ˜¾çš„æ•…éšœä¼ æ’­è·¯å¾„ï¼Œå¯èƒ½æ˜¯å­¤ç«‹äº‹ä»¶"
            return "æœªå‘çŽ°æ•…éšœä¼ æ’­è·¯å¾„"
        
        # Step 4: Analyze propagation scope
        scope_analysis = self.propagation_analyzer.quantify_propagation_scope(all_paths)
        yield f"ðŸ“ˆ å½±å“èŒƒå›´åˆ†æž: å…±å½±å“ {scope_analysis['total_affected_nodes']} ä¸ªèŠ‚ç‚¹"
        
        # Step 5: Generate comprehensive analysis
        final_analysis = self._generate_comprehensive_analysis(
            alert_info, initial_analysis, all_paths, scope_analysis
        )
        
        yield "âœ… è¯Šæ–­å®Œæˆ"
        return final_analysis
    
    def generate_remediation_plan(self, diagnosis_result: str, 
                                propagation_paths: List[PropagationPath],
                                scope_analysis: Dict[str, Any]) -> str:
        """
        Generate detailed remediation plan based on diagnosis results.
        
        Args:
            diagnosis_result: Result from interactive diagnosis
            propagation_paths: Identified propagation paths
            scope_analysis: Scope analysis results
        
        Returns:
            Comprehensive remediation plan
        """
        # Prepare data for remediation prompt
        paths_data = [path.to_dict() for path in propagation_paths]
        
        prompt = self.remediation_prompt.format(
            root_cause_analysis=diagnosis_result,
            propagation_paths=json.dumps(paths_data, indent=2, ensure_ascii=False),
            scope_analysis=json.dumps(scope_analysis, indent=2, ensure_ascii=False)
        )
        
        response = self.llm.invoke(prompt)
        return response.content
    
    def _extract_anomaly_nodes_from_alert(self, alert_info: str) -> List[str]:
        """
        Extract potential anomaly nodes from alert information.
        
        This is a simple heuristic-based extraction. In production,
        this could be enhanced with NER models.
        """
        import re
        
        nodes = []
        
        # Look for service names
        service_pattern = r'service[:\s]+([\w-]+)'
        services = re.findall(service_pattern, alert_info, re.IGNORECASE)
        nodes.extend([f"service:{s}" for s in services])
        
        # Look for pod names
        pod_pattern = r'pod[:\s]+([\w-]+)'
        pods = re.findall(pod_pattern, alert_info, re.IGNORECASE)
        nodes.extend([f"pod:{p}" for p in pods])
        
        # Look for node names
        node_pattern = r'node[:\s]+([\w-]+)'
        node_names = re.findall(node_pattern, alert_info, re.IGNORECASE)
        nodes.extend([f"node:{n}" for n in node_names])
        
        # If no specific nodes found, try to extract any entity names
        if not nodes:
            # Look for common Kubernetes resource patterns
            entity_pattern = r'([\w-]+(?:-[\w]+)*(?:-\d+)?(?:-[a-f0-9]{5,})?)'  
            entities = re.findall(entity_pattern, alert_info)
            # Filter out common words and keep likely resource names
            likely_resources = [e for e in entities if len(e) > 3 and '-' in e]
            nodes.extend([f"entity:{e}" for e in likely_resources[:3]])  # Limit to 3
        
        return list(set(nodes))  # Remove duplicates
    
    def _generate_comprehensive_analysis(self, alert_info: str, initial_analysis: str,
                                       propagation_paths: List[PropagationPath],
                                       scope_analysis: Dict[str, Any]) -> str:
        """
        Generate comprehensive analysis combining all diagnosis results.
        """
        analysis_parts = [
            "# æ•…éšœè¯Šæ–­ç»¼åˆåˆ†æžæŠ¥å‘Š",
            f"\n## å‘Šè­¦ä¿¡æ¯\n{alert_info}",
            f"\n## åˆæ­¥åˆ†æž\n{initial_analysis}",
            f"\n## ä¼ æ’­è·¯å¾„åˆ†æž\nå‘çŽ° {len(propagation_paths)} æ¡æ•…éšœä¼ æ’­è·¯å¾„:"
        ]
        
        # Add path summaries
        for i, path in enumerate(propagation_paths[:3]):  # Show top 3 paths
            summary = path.get_path_summary()
            analysis_parts.append(
                f"\n### è·¯å¾„ {i+1} (æ¦‚çŽ‡: {summary['probability']:.2f})\n"
                f"- æŒç»­æ—¶é—´: {summary['total_duration']:.1f} ç§’\n"
                f"- å½±å“èŠ‚ç‚¹: {summary['node_count']} ä¸ª\n"
                f"- ä¸¥é‡ç¨‹åº¦å˜åŒ–: {' â†’ '.join(summary['severity_progression'])}"
            )
        
        # Add scope analysis
        analysis_parts.extend([
            f"\n## å½±å“èŒƒå›´åˆ†æž",
            f"- æ€»å½±å“èŠ‚ç‚¹æ•°: {scope_analysis['total_affected_nodes']}",
            f"- å½±å“èŒƒå›´ä¸¥é‡ç¨‹åº¦: {scope_analysis['scope_severity']}",
            f"- ä¼ æ’­é€Ÿåº¦: {scope_analysis['propagation_velocity']:.2f} èŠ‚ç‚¹/ç§’"
        ])
        
        # Add node type breakdown
        node_types = scope_analysis.get('node_type_counts', {})
        if node_types:
            analysis_parts.append("\n### å—å½±å“ç»„ä»¶ç±»åž‹:")
            for node_type, count in node_types.items():
                analysis_parts.append(f"- {node_type}: {count} ä¸ª")
        
        # Add core components
        core_components = scope_analysis.get('core_components', [])
        if core_components:
            analysis_parts.append("\n### æ ¸å¿ƒå½±å“ç»„ä»¶:")
            for comp in core_components[:3]:
                analysis_parts.append(
                    f"- {', '.join(comp['nodes'])} (ä¸­å¿ƒæ€§å¾—åˆ†: {comp['centrality_score']:.2f})"
                )
        
        return "\n".join(analysis_parts)
    
    def get_cth_query_interface(self) -> Dict[str, Any]:
        """
        Get interface for querying CTH graph (for tool integration).
        
        Returns:
            Dictionary of available query methods and their descriptions
        """
        return {
            'query_nodes_by_entity': {
                'description': 'æ ¹æ®å®žä½“åç§°æŸ¥è¯¢ç›¸å…³çš„è¶…è¾¹',
                'parameters': ['entity_name', 'time_range (optional)']
            },
            'query_anomalous_events': {
                'description': 'æŸ¥è¯¢å¼‚å¸¸äº‹ä»¶å’Œç›¸å…³è¶…è¾¹',
                'parameters': ['severity_level (optional)', 'time_range (optional)']
            },
            'find_propagation_paths': {
                'description': 'æŸ¥æ‰¾ä»ŽæŒ‡å®šèŠ‚ç‚¹å¼€å§‹çš„æ•…éšœä¼ æ’­è·¯å¾„',
                'parameters': ['start_node', 'max_paths (optional)']
            },
            'get_graph_statistics': {
                'description': 'èŽ·å–CTHå›¾çš„ç»Ÿè®¡ä¿¡æ¯',
                'parameters': []
            }
        }
    
    def query_cth_graph(self, query_type: str, **kwargs) -> Dict[str, Any]:
        """
        Query CTH graph with specified parameters.
        
        This method serves as a tool interface for LLM agents.
        
        Args:
            query_type: Type of query to perform
            **kwargs: Query parameters
        
        Returns:
            Query results
        """
        if not self.current_cth_graph:
            return {'error': 'No CTH graph available'}
        
        try:
            if query_type == 'query_nodes_by_entity':
                entity_name = kwargs.get('entity_name', '')
                hyperedges = self.current_cth_graph.get_hyperedges_containing(entity_name)
                return {
                    'entity': entity_name,
                    'hyperedges_count': len(hyperedges),
                    'hyperedges': [edge.to_dict() for edge in hyperedges]
                }
            
            elif query_type == 'query_anomalous_events':
                severity = kwargs.get('severity_level', None)
                anomalous_edges = []
                for edge in self.current_cth_graph.hyperedges.values():
                    if severity is None or edge.severity == severity:
                        if edge.metrics or edge.logs or edge.severity != 'normal':
                            anomalous_edges.append(edge)
                
                return {
                    'anomalous_events_count': len(anomalous_edges),
                    'events': [edge.to_dict() for edge in anomalous_edges]
                }
            
            elif query_type == 'find_propagation_paths':
                start_node = kwargs.get('start_node', '')
                max_paths = kwargs.get('max_paths', 5)
                paths = self.propagation_analyzer.find_propagation_paths(
                    self.current_cth_graph, start_node, max_paths
                )
                return {
                    'start_node': start_node,
                    'paths_found': len(paths),
                    'paths': [path.to_dict() for path in paths]
                }
            
            elif query_type == 'get_graph_statistics':
                return self.current_cth_graph.get_statistics()
            
            else:
                return {'error': f'Unknown query type: {query_type}'}
        
        except Exception as e:
            return {'error': f'Query failed: {str(e)}'}